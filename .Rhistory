if (filter == FALSE) {
buffett_small_corpus = data}
buffett_small_dfm <- dfm(buffett_small_corpus,
groups = "year",
remove = stopwords("english"),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE)
# Calculate keyness
result_keyness1 <- textstat_keyness(buffett_small_dfm,
tbuffettget = "FINMA")
# Keyness Plot
textplot_keyness(result_keyness1)
}
get_keyness_plot(buffett_corpus, "1977", "1978", TRUE)
get_keyness_plot(buffett_corpus, "", "", FALSE)
get_keyness_plot(buffett_corpus, "2006", "2008", TRUE)
get_topn <- function(chosen_dict, n, type){
# perform dfm analysis based on chosen dictionary
buffett_corpus_lexi <- buffett_corpus %>%
filter(year %in% years) %>%
dfm(.,
groups = "year",
remove = stopwords("english"),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
dictionary = chosen_dict)
# data cleaning and transformations
df_lex <- convert(buffett_corpus_lexi, to = "data.frame")
df_lex$doc_id <- as.numeric(df_lex$doc_id)
rownames(df_lex) <- df_lex$doc_id
df_lex <- df_lex[,-1]
col_sums <- data.frame(colSums(df_lex)) # count columns
colnames(col_sums) <- "counts" # change colnames
col_sums <- arrange(col_sums, counts) # sort it ascending
topn_counts <- tail(col_sums, n) # get the top n values
topn_counts$topics <- rownames(topn_counts)
# select the top n topics
df_lex <- df_lex %>%
select(topn_counts$topics)
if(type == "percentage") {
# calculate percentages and get back the year
df_lex <- df_lex/rowSums(df_lex)*100
}
# from short to long dataframe for plotting
df_lex$year <- as.numeric(rownames(df_lex))
df_lex <- df_lex %>%
gather(key = "topic", value = count, -year)
# plot the Top n Topics
if(type == "percentage") {
ggplot(df_lex, aes(x = year, y = count, fill = topic)) +
geom_area() +
labs(title= paste0("Change in Top " , n, " topics over the years | ", deparse(substitute(chosen_dict)))) +
ylab('percentage')
} else {
ggplot(df_lex, aes(x = year, y = count)) +
geom_line(aes(color = topic, linetype = topic)) +
labs(title= paste0("Change in Top " , n, " topics over the years | ", deparse(substitute(chosen_dict)))) +
ylab('count')}
}
# Define the dictionaries for topics
dict_lexi <- dictionary(file=("./L6/policy_agendas_english.lcd"))
dict_lm_without <- data_dictionary_LoughranMcDonald[-c(1:2,8:9)] # without negative and positive and modal words
dict_nrc_without <- data_dictionary_NRC[-c(6:7)] # without negative and positive
# Define the dictionaries for negative and postive
dict_lm <- data_dictionary_LoughranMcDonald[c(1:2)]
dict_afinn <- data_dictionary_AFINN
dict_nrc <- data_dictionary_NRC[c(6:7)]
# Run the plots for the various dictionaries
get_topn(dict_lexi, 5, "absolute")
get_topn(dict_lm_without, 5, "absolute")
get_topn(dict_nrc_without, 5, "absolute" )
get_topn(dict_lm, 2, "absolute")
get_topn(dict_afinn, 2, "absolute")
get_topn(dict_nrc, 2, "absolute")
get_topn(dict_lexi, 5, "percentage")
get_topn(dict_lm_without, 5, "percentage")
get_topn(dict_nrc_without, 5, "percentage" )
get_topn(dict_lm, 2, "percentage")
get_topn(dict_afinn, 2, "percentage")
get_topn(dict_nrc, 2, "percentage")
lexicon::hash_valence_shifters
# Polarity with valence shifters
sentiment(text.var = buffett_corpus,
polarity_dt = lexicon::hash_sentiment_jockers_rinker,
n.before = 5,
n.after = 2)
library(magrittr)
presidential_debates_2012
presidential_debates_2012 %>%
dplyr::mutate(dialogue_split = get_sentences(dialogue)) %$%
sentiment_by(dialogue_split, list(person, time))
# Transform to dataframe
buffet_df <- data.frame(text = sapply(buffett_corpus, as.character), stringsAsFactors = FALSE)
buffet_df$year <- seq(1977, 2020)
# Make it tidy and join senting with BING lexicon
tidy_letters <- buffet_df %>%
tidytext::unnest_tokens(word, text) %>%   # split text into words
dplyr::anti_join(stop_words, by = "word") %>% # remove stop words
filter(!grepl('[0-9]', word)) %>% # filter out numbers
left_join(get_sentiments("bing"), by = "word") %>%  # add sentiment scores to words
mutate(sentiment = ifelse(is.na(sentiment), "neutral", sentiment)) %>%
group_by(year) %>%
ungroup()
# Calculate sentiment score by letter
letters_sentiment <- tidy_letters %>%
count(year, sentiment) %>%
spread(key = sentiment, value = n) %>%
mutate(sentiment_pct = (positive - negative) / (positive + negative + neutral)) %>%
select(year, sentiment_pct)
ggplot(letters_sentiment, aes(x = year, y = sentiment_pct)) +
geom_bar(aes(fill = sentiment_pct < 0), stat = 'identity') +
geom_text(aes(label = year, hjust = ifelse(sentiment_pct >= 0, -0.15, 1.15)), vjust = 0.5) +
scale_fill_manual(guide = F, values = c('#565b63', '#c40909')) +
coord_flip() +
labs(y='Net Sentiment Ratio',
title='Text sentiment of shareholder letters with BING lexicon')
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 1000))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
#merge both dataframes
df <- full_join(df3, df4)
range(df$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(df3$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(df$publication_date < as.Date(min_date)), ]# it does
nrow(df)
View(df)
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(tidyverse)
library(lubridate)
rm(list=ls())
deglaze <- function(page_id) {
page_data <- read_html(paste('http://www.presidency.ucsb.edu/ws/index.php?pid=', page_id, sep = ''))
page_title <- page_data %>%
html_node('title') %>%
html_text()
page_date <- page_data %>%
html_node('.docdate') %>%
html_text() %>%
mdy() %>%
as.character()
page_text <- page_data %>%
html_nodes('p') %>%
html_text() %>%
paste(collapse = ' ')
return(as_tibble(cbind(title = page_title, date = page_date, text = page_text)))
}
# function to extract text from hyperlink (as character)
extract_link_title <- function(anchor_tag) {
return(unlist(strsplit(unlist(strsplit(anchor_tag, '>'))[2], '<'))[1])
}
# function to extract target URL from hyperlink (as character)
extract_link_page_id <- function(anchor_tag) {
return(c(unlist(strsplit(unlist(strsplit(anchor_tag, '?pid='))[2], '\">'))[1]))
}
# function to extract president's name from title
extract_president <- function(title) {
return(unlist(strsplit(title, ':'))[1])
}
exec_links <- read_html('http://www.presidency.ucsb.edu/executive_orders.php?year=2017&Submit=DISPLAY') %>%
html_nodes('a') %>%
as.character() %>%
as_tibble() %>%
unique() %>%
filter(grepl('../ws/index.php?pid=', value, fixed = TRUE)) %>%
mutate(title = mapply(extract_link_title, value),
page_id = mapply(extract_link_page_id, value)) %>%
select(title, page_id)
trump_orders <- mapply(deglaze, exec_links$page_id) %>%
t() %>%
as_tibble() %>%
unnest() %>%
mutate(president = mapply(extract_president, title)) %>%
filter(president == 'Donald J. Trump')
Truman <- read_html("http://www.presidency.ucsb.edu/data/popularity.php?pres=33")
Truman %>%
html_table(fill=T) -> Truman
Truman <- read_html("https://www.presidency.ucsb.edu/documents/app-categories/presidential/memoranda?page=2")
Truman %>%
html_table(fill=T) -> Truman
Truman <- read_html("https://www.presidency.ucsb.edu/documents/app-categories/presidential/memoranda?page=2")
View(Truman)
Truman %>%
html_table(fill=T) -> Truman2
library(requests)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&category2%5B%5D=58&items_per_page=100
url = 'https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&category2%5B%5D=58&items_per_page=100'
GET(url)
r <- GET(url)
data = fromJSON(rawToChar(r$content))
View(r)
data = fromJSON((r$content))
data = fromJSON((r))
r$content
rawToChar(r$content)
fromJSON(rawToChar(r$content))
r <- (rawToChar(r$content))
(rawToChar(r$content))
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
url = 'https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&category2%5B%5D=58&items_per_page=100'
r <- GET(url)
data = fromJSON((r))
(rawToChar(r$content))
read_html(r)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 1000))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
View(df)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 20))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
# get all other document types
res = GET(paste('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data4 = fromJSON((rawToChar(res$content)))
# get all other document types
res = GET(paste('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
res
GET('https://www.federalregister.gov/api/v1/documents.json?per_page=20&conditions%5Bpresidential_document_type%5D%5B%5D=presidential_order')
# get all other document types
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
View(df4)
# get all presidential orders
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
res <- GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data5 = fromJSON((rawToChar(res$content)))
df5 <- data.frame(data5$results)
# get all memorandums
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data5 = fromJSON((rawToChar(res$content)))
df5 <- data.frame(data5$results)
View(df5)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 20))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order (for testing purposes)
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
# get all presidential orders
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data5 = fromJSON((rawToChar(res$content)))
presidential.orders <- data.frame(data5$results)
# get all memorandums
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data6 = fromJSON((rawToChar(res$content)))
memorandums <- data.frame(data6$results)
# get all notices
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data7 = fromJSON((rawToChar(res$content)))
notices <- data.frame(data7$results)
# get all proclamations
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data8 = fromJSON((rawToChar(res$content)))
proclamations <- data.frame(data8$results)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 20))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order (for testing purposes)
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
doc.types = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data = fromJSON(rawToChar((res$content)))
exec.order1 <- data.frame(data$results)
range(exec.order1$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data = fromJSON((rawToChar(res$content)))
exec.order2 <- data.frame(data$results)
# get all presidential orders
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data = fromJSON((rawToChar(res$content)))
presidential.orders <- data.frame(data$results)
# get all memorandums
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
memorandums <- data.frame(data$results)
# get all notices
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
notices <- data.frame(data$results)
# get all proclamations
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
proclamations <- data.frame(data$results)
#merge the dataframes for executive orders
exec.orders <- full_join(exec.order1, exec.order2)
range(exec.orders$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(exec.order1$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(exec.orders$publication_date < as.Date(min_date)), ]# it does
# download all pdfs
dir.create('./data/executive_orders')
dir.create('./data/presidential_orders')
dir.create('./data/memorandums')
dir.create('./data/notices')
dir.create('./data/proclamations')
notices
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 20))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order (for testing purposes)
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
doc.types = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data = fromJSON(rawToChar((res$content)))
exec.order1 <- data.frame(data$results)
range(exec.order1$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data = fromJSON((rawToChar(res$content)))
exec.order2 <- data.frame(data$results)
# get all presidential orders
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data = fromJSON((rawToChar(res$content)))
presidential.orders <- data.frame(data$results)
# get all memorandums
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
memorandums <- data.frame(data$results)
# get all notices
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
notices <- data.frame(data$results)
# get all proclamations
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
proclamations <- data.frame(data$results)
### MERGE DATA -----------------------------------
#merge the dataframes for executive orders
exec.orders <- full_join(exec.order1, exec.order2)
range(exec.orders$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(exec.order1$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(exec.orders$publication_date < as.Date(min_date)), ]# it does
### SAVE DATA -----------------------------------
# download all pdfs
dir.create('./data/executive_orders')
dir.create('./data/presidential_orders')
dir.create('./data/memorandums')
dir.create('./data/notices')
dir.create('./data/proclamations')
# NOTE: CHANGE MODE = 'WB' FOR WINDOWS AND LEAVE IT OUT FOR MAC
# # save executive orders
# for (i in 1:nrow(exec.orders)){
#   name = exec.orders$document_number[i]
#   tryCatch(download(exec.orders$pdf_url[i], destfile = paste0('./data/executive_orders/', name, '.pdf'), timeout = 1000, mode = "wb"),
#            error = function(e) print(paste(name, e)))
# }
# save presidential orders
for (i in 1:nrow(exec.orders)){
name = presidential.orders$document_number[i]
tryCatch(download(presidential.orders$pdf_url[i], destfile = paste0('./data/presidential_orders/', name, '.pdf'), timeout = 1000, mode = "wb"),
error = function(e) print(paste(name, e)))
}
# save memorandums
for (i in 1:nrow(exec.orders)){
name = memorandums$document_number[i]
tryCatch(download(memorandums$pdf_url[i], destfile = paste0('./data/memorandums/', name, '.pdf'), timeout = 1000, mode = "wb"),
error = function(e) print(paste(name, e)))
}
