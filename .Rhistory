#keeping only data with no missing values
documents <- filter(documents, !is.na(date))
## Inspecting the result ----
nrow(documents)
range(nchar(documents$EO_nr), na.rm = TRUE)
sum(is.na(documents$EO_nr))
sum(is.na(documents$date))
sum(is.na(documents$EO_nr) | is.na(documents$date))
#keeping only data with no missing values
documents <- filter(documents, !is.na(date))
#importing a list of presidential documents sorted by president. contains the var "document_type"
list_of_documents_by_clinton <- read.csv("https://www.federalregister.gov/documents/search.csv?conditions%5Bcorrection%5D=0&conditions%5Bpresident%5D=william-j-clinton&conditions%5Bpresidential_document_type%5D%5B%5D=determination&conditions%5Bpresidential_document_type%5D%5B%5D=memorandum&conditions%5Bpresidential_document_type%5D%5B%5D=notice&conditions%5Bpresidential_document_type%5D%5B%5D=presidential_order&conditions%5Bsigning_date%5D%5Byear%5D=&conditions%5Btype%5D%5B%5D=PRESDOCU&fields%5B%5D=citation&fields%5B%5D=document_number&fields%5B%5D=end_page&fields%5B%5D=html_url&fields%5B%5D=pdf_url&fields%5B%5D=type&fields%5B%5D=subtype&fields%5B%5D=publication_date&fields%5B%5D=signing_date&fields%5B%5D=start_page&fields%5B%5D=title&fields%5B%5D=disposition_notes&order=document_number&per_page=1000")
list_of_documents_by_w.bush <- read.csv("https://www.federalregister.gov/documents/search.csv?conditions%5Bcorrection%5D=0&conditions%5Bpresident%5D=george-w-bush&conditions%5Bpresidential_document_type%5D%5B%5D=determination&conditions%5Bpresidential_document_type%5D%5B%5D=memorandum&conditions%5Bpresidential_document_type%5D%5B%5D=notice&conditions%5Bpresidential_document_type%5D%5B%5D=presidential_order&conditions%5Bsigning_date%5D%5Byear%5D=&conditions%5Btype%5D%5B%5D=PRESDOCU&fields%5B%5D=citation&fields%5B%5D=document_number&fields%5B%5D=end_page&fields%5B%5D=html_url&fields%5B%5D=pdf_url&fields%5B%5D=type&fields%5B%5D=subtype&fields%5B%5D=publication_date&fields%5B%5D=signing_date&fields%5B%5D=start_page&fields%5B%5D=title&fields%5B%5D=disposition_notes&order=document_number&per_page=1000")
list_of_documents_by_obama <- read.csv("https://www.federalregister.gov/documents/search.csv?conditions%5Bcorrection%5D=0&conditions%5Bpresident%5D=barack-obama&conditions%5Bpresidential_document_type%5D%5B%5D=determination&conditions%5Bpresidential_document_type%5D%5B%5D=memorandum&conditions%5Bpresidential_document_type%5D%5B%5D=notice&conditions%5Bpresidential_document_type%5D%5B%5D=presidential_order&conditions%5Bsigning_date%5D%5Byear%5D=&conditions%5Btype%5D%5B%5D=PRESDOCU&fields%5B%5D=citation&fields%5B%5D=document_number&fields%5B%5D=end_page&fields%5B%5D=html_url&fields%5B%5D=pdf_url&fields%5B%5D=type&fields%5B%5D=subtype&fields%5B%5D=publication_date&fields%5B%5D=signing_date&fields%5B%5D=start_page&fields%5B%5D=title&fields%5B%5D=disposition_notes&order=document_number&per_page=1000")
list_of_documents_by_trump <- read.csv("https://www.federalregister.gov/documents/search.csv?conditions%5Bcorrection%5D=0&conditions%5Bpresident%5D=donald-trump&conditions%5Bpresidential_document_type%5D%5B%5D=determination&conditions%5Bpresidential_document_type%5D%5B%5D=memorandum&conditions%5Bpresidential_document_type%5D%5B%5D=notice&conditions%5Bpresidential_document_type%5D%5B%5D=presidential_order&conditions%5Bsigning_date%5D%5Byear%5D=&conditions%5Btype%5D%5B%5D=PRESDOCU&fields%5B%5D=citation&fields%5B%5D=document_number&fields%5B%5D=end_page&fields%5B%5D=html_url&fields%5B%5D=pdf_url&fields%5B%5D=type&fields%5B%5D=subtype&fields%5B%5D=publication_date&fields%5B%5D=signing_date&fields%5B%5D=start_page&fields%5B%5D=title&fields%5B%5D=disposition_notes&order=document_number&per_page=1000")
list_of_documents_by_biden <- read.csv("https://www.federalregister.gov/documents/search.csv?conditions%5Bcorrection%5D=0&conditions%5Bpresident%5D=joe-biden&conditions%5Bpresidential_document_type%5D%5B%5D=determination&conditions%5Bpresidential_document_type%5D%5B%5D=memorandum&conditions%5Bpresidential_document_type%5D%5B%5D=notice&conditions%5Bpresidential_document_type%5D%5B%5D=presidential_order&conditions%5Bsigning_date%5D%5Byear%5D=&conditions%5Btype%5D%5B%5D=PRESDOCU&fields%5B%5D=citation&fields%5B%5D=document_number&fields%5B%5D=end_page&fields%5B%5D=html_url&fields%5B%5D=pdf_url&fields%5B%5D=type&fields%5B%5D=subtype&fields%5B%5D=publication_date&fields%5B%5D=signing_date&fields%5B%5D=start_page&fields%5B%5D=title&fields%5B%5D=disposition_notes&order=document_number&per_page=1000")
#adding a new var "president" to the "documents" object by comparing "document_number"
documents <- documents %>%
mutate(president = as.factor(case_when(
(EO_nr >= 12945 & EO_nr <= 13197) | document_number %in% list_of_documents_by_clinton$document_number ~ "Clinton",
(EO_nr >= 13198 & EO_nr <= 13488) | document_number %in% list_of_documents_by_w.bush$document_number ~ "W. Bush",
(EO_nr >= 13489 & EO_nr <= 13764) | document_number %in% list_of_documents_by_obama$document_number ~ "Obama",
(EO_nr >= 13765 & EO_nr <= 13984) | document_number %in% list_of_documents_by_trump$document_number ~ "Trump",
(EO_nr >= 13985) | document_number %in% list_of_documents_by_biden$document_number ~ "Biden",
)))
#remove lists to unclutter the environment
rm(
list_of_documents_by_clinton,
list_of_documents_by_w.bush,
list_of_documents_by_obama,
list_of_documents_by_trump,
list_of_documents_by_biden
)
View(documents)
collocations <- corp_main %>%
textstat_collocations(min_count = 100,
size = 2) %>%
filter(count >100)
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
library(newsmap)
library(sentimentr)
library(data.table)
collocations
collocations <- corp_main %>%
textstat_collocations(min_count = 100,
size = 2) %>%
filter(count >100)
corp_main <- tokens_compound(corp_main,
phrase(collocations$collocation))
# The following code creates the already cleaned main corpus for our analysis.
corp_main <- corpus(documents)
corp_main <- tokens(corp_main,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_remove(stopwords("english"))
number_corp_main<-ntoken(corp_main) # By adding the number of tokens to our dataframe documents, we get a fealing of the length of each EO.
documents<-cbind(documents,number_corp_main)
collocations <- corp_main %>%
textstat_collocations(min_count = 100,
size = 2) %>%
filter(count >100)
corp_main <- tokens_compound(corp_main,
phrase(collocations$collocation))
View(collocations)
# The following code creates the already cleaned main corpus for our analysis.
corp_main <- corpus(documents)
corp_main <- tokens(corp_main,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_remove(stopwords("english"))
# The following code creates the already cleaned main corpus for our analysis.
corp_main <- corpus(documents)
corp_main_tokens <- tokens(corp_main,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"))
number_corp_main<-ntoken(corp_main_tokens) # By adding the number of tokens to our dataframe documents, we get a fealing of the length of each EO.
documents<-cbind(documents,number_corp_main)
View(documents)
# The following code creates the already cleaned main corpus for our analysis.
corp_main <- corpus(documents)
corp_main_tokens <- tokens(corp_main,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"))
number_corp_main<-ntoken(corp_main_tokens) # By adding the number of tokens to our dataframe documents, we get a fealing of the length of each EO.
documents<-cbind(documents,number_corp_main)
collocations <- corp_main_tokens %>%
textstat_collocations(min_count = 250,
size = 2) %>%
filter(count >250)
corp_main_tokens <- tokens_compound(corp_main_tokens,
phrase(collocations$collocation))
corp_main_dfm<-dfm(corp_main_tokens)
#remove Data and values to unclutter the environment
rm(
dfmat_feat,
dfmat_feat_select,
dfmat_label,
tokens_corp1,
toks_label,
collocations,
corp_main_tokens,
tmod_nm,
day,
month,
corp1,
number_corp_main,
pred_nm,
prediction_country,
USA,
number_corp_main
)
View(documents)
# The following code creates the already cleaned main corpus for our analysis.
corp_main <- corpus(documents)
corp_main_tokens <- tokens(corp_main,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"))
number_corp_main<-ntoken(corp_main_tokens) # By adding the number of tokens to our dataframe documents, we get a fealing of the length of each EO.
documents<-cbind(documents,number_corp_main)
collocations <- corp_main_tokens %>%
textstat_collocations(min_count = 250,
size = 2) %>%
filter(count >250)
View(collocations)
corp_main_tokens <- tokens_compound(corp_main_tokens,
phrase(collocations$collocation))
corp_main_dfm<-dfm(corp_main_tokens)
number_corp_main<-ntoken(corp_main_tokens) # By adding the number of tokens to our dataframe documents, we get a fealing of the length of each EO.
documents<-cbind(documents,number_corp_main)
collocations <- corp_main_tokens %>%
textstat_collocations(min_count = 250,
size = 2) %>%
filter(count >250)
corp_main_tokens <- tokens_compound(corp_main_tokens,
phrase(collocations$collocation))
corp_main_dfm<-dfm(corp_main_tokens)
#remove Data and values to unclutter the environment
rm(
dfmat_feat,
dfmat_feat_select,
dfmat_label,
tokens_corp1,
toks_label,
collocations,
corp_main_tokens,
tmod_nm,
day,
month,
corp1,
number_corp_main,
pred_nm,
prediction_country,
USA,
number_corp_main
)
library(quanteda.dictionaries)
library(quanteda.corpora)
library(quanteda.tidy)
corp_main_dfm_AFINN <- corp_main_dfm %>%
dfm(.,
dictionary = data_dictionary_AFINN)
head(corp_main_dfm_AFINN)
View(corp_main_dfm_AFINN)
View(corp_main_dfm)
View(corp_main_dfm_AFINN)
View(documents)
# The following code creates the already cleaned main corpus for our analysis.
corp_main <- corpus(documents)
corp_main_tokens <- tokens(corp_main,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"))
View(corp_main_tokens)
number_corp_main<-ntoken(corp_main_tokens) # By adding the number of tokens to our dataframe documents, we get a fealing of the length of each EO.
documents<-cbind(documents,number_corp_main)
collocations <- corp_main_tokens %>%
textstat_collocations(min_count = 250,
size = 2) %>%
filter(count >250)
corp_main_tokens <- tokens_compound(corp_main_tokens,
phrase(collocations$collocation))
corp_main_dfm<-dfm(corp_main_tokens)
#remove Data and values to unclutter the environment
rm(
dfmat_feat,
dfmat_feat_select,
dfmat_label,
tokens_corp1,
toks_label,
collocations,
corp_main_tokens,
tmod_nm,
day,
month,
corp1,
number_corp_main,
pred_nm,
prediction_country,
USA,
number_corp_main
)
corp_main_dfm_AFINN <- corp_main_dfm %>%
dfm(.,
dictionary = data_dictionary_AFINN)
head(corp_main_dfm_AFINN)
result <- corp_main_dfm_AFINN %>%
positive-negative
# The following code creates the already cleaned main corpus for our analysis.
corp_main <- corpus(documents)
corp_main_tokens <- tokens(corp_main,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"))
number_corp_main<-ntoken(corp_main_tokens) # By adding the number of tokens to our dataframe documents, we get a fealing of the length of each EO.
documents<-cbind(documents,number_corp_main)
head(corp_main_tokens)
head(summary(corp_main))
# The following code creates the already cleaned main corpus for our analysis.
corp_main <- corpus(documents)
corp_main_tokens <- tokens(corp_main,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"))
number_corp_main<-ntoken(corp_main_tokens) # By adding the number of tokens to our dataframe documents, we get a fealing of the length of each EO.
documents<-cbind(documents,number_corp_main)
collocations <- corp_main_tokens %>%
textstat_collocations(min_count = 250,
size = 2) %>%
filter(count >250)
corp_main_tokens <- tokens_compound(corp_main_tokens,
phrase(collocations$collocation))
corp_main_dfm<-dfm(corp_main_tokens)
# Using Newsmap
corp1 <- corpus(documents) # corpus for Newsmap
month <- c("January", "February", "March", "April", "May", "June","July", "August", "September", "October", "November", "December")
day <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday","Sunday")
USA <- c("States", "Sec", "United","Act","Secretary","Council","State","Department","General","Section","Management","America","Committee","American","Americans","Washington")
tokens_corp1 <- tokens(corp1,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_remove(c(stopwords("english"),
month,
day,
USA))
toks_label <- tokens_lookup(tokens_corp1,
dictionary = data_dictionary_newsmap_en,
levels = 3) # level 3 stands for countries
dfmat_label <- dfm(toks_label, tolower = FALSE)
dfmat_feat <- dfm(tokens_corp1, tolower = FALSE)
dfmat_feat_select <- dfm_select(dfmat_feat, pattern = "^[A-Z][A-Za-z0-9]+",
valuetype = "regex", case_insensitive = FALSE) %>%
dfm_trim(min_termfreq = 10)
tmod_nm <- textmodel_newsmap(dfmat_feat_select, y = dfmat_label) # Training the Newsmap model
coef(tmod_nm,n=15)[c("US","CN","IQ")] # Extraction of model coefficients
pred_nm <- predict(tmod_nm) # Prediction of country labels on our documents
prediction_country<-table(pred_nm) # Frequency of countries in EO
prediction_country
documents<-cbind(documents,pred_nm) # joining documents with predicted country labels
#remove Data and values to unclutter the environment
rm(
dfmat_feat,
dfmat_feat_select,
dfmat_label,
tokens_corp1,
toks_label,
collocations,
corp_main_tokens,
tmod_nm,
day,
month,
corp1,
number_corp_main,
pred_nm,
prediction_country,
USA,
number_corp_main
)
corp_main_dfm_AFINN <- corp_main_dfm %>%
dfm(.,
dictionary = data_dictionary_AFINN)
head(corp_main_dfm_AFINN)
View(corp_main_dfm_AFINN)
select(corp_main_dfm_AFINN, positive)
dfm_select(corp_main_dfm_AFINN)
dfm_select(corp_main_dfm_AFINN,positive)
emotion <- as.data.frame(corp_main_dfm_AFINN)
emotion <- convert(corp_main_dfm_AFINN, to = "data.frame")
View(emotion)
net_emotion <- emotion$positive-emotion$negative
cbind(documents,net_emotion)
View(documents)
documents <- cbind(documents,net_emotion)
View(documents)
rm(
emotioin,
net_emotion
corp_main_dfm_AFINN
)
rm(
emotioin,
net_emotion,
corp_main_dfm_AFINN
)
rm(
emotion,
net_emotion,
corp_main_dfm_AFINN
)
#remove Data and values to unclutter the environment
rm(
dfmat_feat,
dfmat_feat_select,
dfmat_label,
tokens_corp1,
toks_label,
collocations,
corp_main_tokens,
corp_main,
tmod_nm,
day,
month,
corp1,
number_corp_main,
pred_nm,
prediction_country,
USA,
number_corp_main
)
library(here)
library(readtext)
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.corpora)
library(quanteda.tidy)
library(stringr)
library(dplyr)
library(newsmap)
library(sentimentr)
library(data.table)
rm(list=ls())
Sys.setenv(lang = "ENG")
Sys.setlocale("LC_ALL", "English") #not setting this to English will break as.Date()
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# as these pdf files are corrupt we delete them
file.remove('./data/executive_orders/00-31252.pdf')
file.remove('./data/executive_orders/2018-04860.pdf')
# function to read in data
read_pdfs <- function (folder_dir){
readtext(paste0('./data/', folder_dir),
docvarsfrom = "filenames",
docvarnames = 'document_number',
ignore_missing_files = TRUE,
verbosity = 3)
}
# read in all data
executive.orders <- read_pdfs('executive_orders/')
presidential.orders <- read_pdfs('presidential_orders/')
memorandums <- read_pdfs('memorandums/')
proclamations <- read_pdfs('proclamations/')
notices <- read_pdfs('notices/')
# merge with existing dfs to get date
executive.orders.df <- fread('./data/dataframes/executive_orders.csv')
presidential.orders.df <- fread('./data/dataframes/presidential_orders.csv')
memorandums.df <- fread('./data/dataframes/memorandums.csv')
proclamations.df <- fread('./data/dataframes/proclamations.csv')
notices.df <- fread('./data/dataframes/notices.csv')
# merge dataframes based on their document_number
executive.orders.df <- left_join(executive.orders.df, executive.orders, on = 'document_number') %>% select(-c(doc_id, abstract, excerpts, type))
presidential.orders.df <- left_join(presidential.orders.df, presidential.orders, on = 'document_number') %>% select(-c(doc_id, abstract, excerpts, type))
memorandums.df <- left_join(memorandums.df, memorandums, on = 'document_number') %>% select(-c(doc_id, abstract, excerpts, type))
proclamations.df <- left_join(proclamations.df, proclamations, on = 'document_number') %>% select(-c(doc_id, abstract, excerpts, type))
notices.df <- left_join(notices.df, notices, on = 'document_number') %>% select(-c(doc_id, abstract, excerpts, type))
# add variable
executive.orders.df$document_type <- 'executive.order'
presidential.orders.df$document_type <- 'presidential.order'
memorandums.df$document_type <- 'memorandum'
proclamations.df$document_type <- 'proclamation'
notices.df$document_type <- 'notice'
# merge all dataframes together
documents <- rbind(executive.orders.df, presidential.orders.df, memorandums.df, proclamations.df, notices.df)
documents <- documents[order(publication_date),]
# count how many NAs we have
sum(is.na(documents$text))
# remove unnecessary values
rm(executive.orders, memorandums, notices, presidential.orders, proclamations)
# check how many unique documents we have
length(unique(documents$document_number))
# The following code creates the already cleaned main corpus for our analysis.
corp_main <- corpus(documents)
corp_main_tokens <- tokens(corp_main,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"))
number_corp_main<-ntoken(corp_main_tokens) # By adding the number of tokens to our dataframe documents, we get a fealing of the length of each EO.
documents<-cbind(documents,number_corp_main)
collocations <- corp_main_tokens %>%
textstat_collocations(min_count = 250,
size = 2) %>%
filter(count >250)
corp_main_tokens <- tokens_compound(corp_main_tokens,
phrase(collocations$collocation))
corp_main_dfm<-dfm(corp_main_tokens)
# Using Newsmap
corp1 <- corpus(documents) # corpus for Newsmap
month <- c("January", "February", "March", "April", "May", "June","July", "August", "September", "October", "November", "December")
day <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday","Sunday")
USA <- c("States", "Sec", "United","Act","Secretary","Council","State","Department","General","Section","Management","America","Committee","American","Americans","Washington")
tokens_corp1 <- tokens(corp1,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_remove(c(stopwords("english"),
month,
day,
USA))
toks_label <- tokens_lookup(tokens_corp1,
dictionary = data_dictionary_newsmap_en,
levels = 3) # level 3 stands for countries
dfmat_label <- dfm(toks_label, tolower = FALSE)
dfmat_feat <- dfm(tokens_corp1, tolower = FALSE)
dfmat_feat_select <- dfm_select(dfmat_feat, pattern = "^[A-Z][A-Za-z0-9]+",
valuetype = "regex", case_insensitive = FALSE) %>%
dfm_trim(min_termfreq = 10)
tmod_nm <- textmodel_newsmap(dfmat_feat_select, y = dfmat_label) # Training the Newsmap model
coef(tmod_nm,n=15)[c("US","CN","IQ")] # Extraction of model coefficients
pred_nm <- predict(tmod_nm) # Prediction of country labels on our documents
prediction_country<-table(pred_nm) # Frequency of countries in EO
prediction_country
documents<-cbind(documents,pred_nm) # joining documents with predicted country labels
#remove Data and values to unclutter the environment
rm(dfmat_feat,dfmat_feat_select,dfmat_label,tokens_corp1,toks_label,collocations,corp_main_tokens,
corp_main,tmod_nm,day,month,corp1,number_corp_main,pred_nm,prediction_country,USA,number_corp_main)
corp_main_dfm_AFINN <- corp_main_dfm %>%
dfm(.,
dictionary = data_dictionary_AFINN)
corp_main_dfm_AFINN <- corp_main_dfm %>%
dfm(.,
dictionary = data_dictionary_AFINN)
emotion <- convert(corp_main_dfm_AFINN, to = "data.frame")
net_emotion <- emotion$positive-emotion$negative
documents <- cbind(documents,net_emotion)
rm(emotion,net_emotion,corp_main_dfm_AFINN)
View(documents)
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- fread('./data/executive_orders_cleaned.csv')
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
library(newsmap)
library(sentimentr)
library(data.table)
library(tidyr)
library(maps)
library(countrycode)
library(ggplot2)
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- fread('./data/executive_orders_cleaned.csv')
library(syuzhet)
library(tidyverse)
library(lubridate)
library(plotly)
library(tibble)
library(dplyr)
data[,"sentiment_EO"]<-NA
data$sentiment_EO <- get_sentiment(data$text, method="afinn")
fig_sentiment <- ggplot(data %>% filter(date > "1950-04-11"),aes(x=date,y=sentiment_EO, color = sentiment_EO)) + geom_point() +
geom_smooth(aes(x=date,y=sentiment_EO),method=lm, se=FALSE)
fig_sentiment
fig_sentiment_China <- ggplot(data %>% filter(country == "China"),aes(x=date,y=sentiment_EO, color = sentiment_EO)) + geom_point() +
geom_smooth(aes(x=date,y=sentiment_EO),method=lm, se=FALSE)
fig_sentiment_China
fig_Obama <- ggplot(data %>% filter(president == "Barack Obama"),aes(x=date,y=sentiment_EO, color = sentiment_EO)) + geom_point() +
geom_smooth(aes(x=date,y=sentiment_EO),method=lm, se=FALSE)
fig_Obama
# frequency of EO by president
presidents <- table(data$president)
presidents_df <- as.data.frame(presidents)
names(presidents_df)[1]="president"
presidents_df
data %>% group_by(president)%>%summarize(total_score=count(eo_number))
