ggplot(letters_sentiment, aes(x = year, y = sentiment_pct)) +
geom_bar(aes(fill = sentiment_pct < 0), stat = 'identity') +
geom_text(aes(label = year, hjust = ifelse(sentiment_pct >= 0, -0.15, 1.15)), vjust = 0.5) +
scale_fill_manual(guide = F, values = c('#565b63', '#c40909')) +
coord_flip() +
labs(y='Net Sentiment Ratio',
title='Text sentiment of shareholder letters with BING lexicon')
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 1000))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
#merge both dataframes
df <- full_join(df3, df4)
range(df$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(df3$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(df$publication_date < as.Date(min_date)), ]# it does
nrow(df)
View(df)
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(tidyverse)
library(lubridate)
rm(list=ls())
deglaze <- function(page_id) {
page_data <- read_html(paste('http://www.presidency.ucsb.edu/ws/index.php?pid=', page_id, sep = ''))
page_title <- page_data %>%
html_node('title') %>%
html_text()
page_date <- page_data %>%
html_node('.docdate') %>%
html_text() %>%
mdy() %>%
as.character()
page_text <- page_data %>%
html_nodes('p') %>%
html_text() %>%
paste(collapse = ' ')
return(as_tibble(cbind(title = page_title, date = page_date, text = page_text)))
}
# function to extract text from hyperlink (as character)
extract_link_title <- function(anchor_tag) {
return(unlist(strsplit(unlist(strsplit(anchor_tag, '>'))[2], '<'))[1])
}
# function to extract target URL from hyperlink (as character)
extract_link_page_id <- function(anchor_tag) {
return(c(unlist(strsplit(unlist(strsplit(anchor_tag, '?pid='))[2], '\">'))[1]))
}
# function to extract president's name from title
extract_president <- function(title) {
return(unlist(strsplit(title, ':'))[1])
}
exec_links <- read_html('http://www.presidency.ucsb.edu/executive_orders.php?year=2017&Submit=DISPLAY') %>%
html_nodes('a') %>%
as.character() %>%
as_tibble() %>%
unique() %>%
filter(grepl('../ws/index.php?pid=', value, fixed = TRUE)) %>%
mutate(title = mapply(extract_link_title, value),
page_id = mapply(extract_link_page_id, value)) %>%
select(title, page_id)
trump_orders <- mapply(deglaze, exec_links$page_id) %>%
t() %>%
as_tibble() %>%
unnest() %>%
mutate(president = mapply(extract_president, title)) %>%
filter(president == 'Donald J. Trump')
Truman <- read_html("http://www.presidency.ucsb.edu/data/popularity.php?pres=33")
Truman %>%
html_table(fill=T) -> Truman
Truman <- read_html("https://www.presidency.ucsb.edu/documents/app-categories/presidential/memoranda?page=2")
Truman %>%
html_table(fill=T) -> Truman
Truman <- read_html("https://www.presidency.ucsb.edu/documents/app-categories/presidential/memoranda?page=2")
View(Truman)
Truman %>%
html_table(fill=T) -> Truman2
library(requests)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&category2%5B%5D=58&items_per_page=100
url = 'https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&category2%5B%5D=58&items_per_page=100'
GET(url)
r <- GET(url)
data = fromJSON(rawToChar(r$content))
View(r)
data = fromJSON((r$content))
data = fromJSON((r))
r$content
rawToChar(r$content)
fromJSON(rawToChar(r$content))
r <- (rawToChar(r$content))
(rawToChar(r$content))
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
url = 'https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&category2%5B%5D=58&items_per_page=100'
r <- GET(url)
data = fromJSON((r))
(rawToChar(r$content))
read_html(r)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 1000))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
View(df)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 20))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
# get all other document types
res = GET(paste('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data4 = fromJSON((rawToChar(res$content)))
# get all other document types
res = GET(paste('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
res
GET('https://www.federalregister.gov/api/v1/documents.json?per_page=20&conditions%5Bpresidential_document_type%5D%5B%5D=presidential_order')
# get all other document types
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
View(df4)
# get all presidential orders
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
res <- GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data5 = fromJSON((rawToChar(res$content)))
df5 <- data.frame(data5$results)
# get all memorandums
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data5 = fromJSON((rawToChar(res$content)))
df5 <- data.frame(data5$results)
View(df5)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 20))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order (for testing purposes)
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
# get all presidential orders
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data5 = fromJSON((rawToChar(res$content)))
presidential.orders <- data.frame(data5$results)
# get all memorandums
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data6 = fromJSON((rawToChar(res$content)))
memorandums <- data.frame(data6$results)
# get all notices
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data7 = fromJSON((rawToChar(res$content)))
notices <- data.frame(data7$results)
# get all proclamations
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data8 = fromJSON((rawToChar(res$content)))
proclamations <- data.frame(data8$results)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 20))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order (for testing purposes)
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
doc.types = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data = fromJSON(rawToChar((res$content)))
exec.order1 <- data.frame(data$results)
range(exec.order1$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data = fromJSON((rawToChar(res$content)))
exec.order2 <- data.frame(data$results)
# get all presidential orders
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data = fromJSON((rawToChar(res$content)))
presidential.orders <- data.frame(data$results)
# get all memorandums
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
memorandums <- data.frame(data$results)
# get all notices
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
notices <- data.frame(data$results)
# get all proclamations
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
proclamations <- data.frame(data$results)
#merge the dataframes for executive orders
exec.orders <- full_join(exec.order1, exec.order2)
range(exec.orders$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(exec.order1$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(exec.orders$publication_date < as.Date(min_date)), ]# it does
# download all pdfs
dir.create('./data/executive_orders')
dir.create('./data/presidential_orders')
dir.create('./data/memorandums')
dir.create('./data/notices')
dir.create('./data/proclamations')
notices
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 20))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order (for testing purposes)
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
doc.types = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data = fromJSON(rawToChar((res$content)))
exec.order1 <- data.frame(data$results)
range(exec.order1$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data = fromJSON((rawToChar(res$content)))
exec.order2 <- data.frame(data$results)
# get all presidential orders
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'presidential_order'))
data = fromJSON((rawToChar(res$content)))
presidential.orders <- data.frame(data$results)
# get all memorandums
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
memorandums <- data.frame(data$results)
# get all notices
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
notices <- data.frame(data$results)
# get all proclamations
res = GET(paste0('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpresidential_document_type%5D%5B%5D=', 'memorandum'))
data = fromJSON((rawToChar(res$content)))
proclamations <- data.frame(data$results)
### MERGE DATA -----------------------------------
#merge the dataframes for executive orders
exec.orders <- full_join(exec.order1, exec.order2)
range(exec.orders$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(exec.order1$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(exec.orders$publication_date < as.Date(min_date)), ]# it does
### SAVE DATA -----------------------------------
# download all pdfs
dir.create('./data/executive_orders')
dir.create('./data/presidential_orders')
dir.create('./data/memorandums')
dir.create('./data/notices')
dir.create('./data/proclamations')
# NOTE: CHANGE MODE = 'WB' FOR WINDOWS AND LEAVE IT OUT FOR MAC
# # save executive orders
# for (i in 1:nrow(exec.orders)){
#   name = exec.orders$document_number[i]
#   tryCatch(download(exec.orders$pdf_url[i], destfile = paste0('./data/executive_orders/', name, '.pdf'), timeout = 1000, mode = "wb"),
#            error = function(e) print(paste(name, e)))
# }
# save presidential orders
for (i in 1:nrow(exec.orders)){
name = presidential.orders$document_number[i]
tryCatch(download(presidential.orders$pdf_url[i], destfile = paste0('./data/presidential_orders/', name, '.pdf'), timeout = 1000, mode = "wb"),
error = function(e) print(paste(name, e)))
}
# save memorandums
for (i in 1:nrow(exec.orders)){
name = memorandums$document_number[i]
tryCatch(download(memorandums$pdf_url[i], destfile = paste0('./data/memorandums/', name, '.pdf'), timeout = 1000, mode = "wb"),
error = function(e) print(paste(name, e)))
}
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
library(newsmap)
library(sentimentr)
library(data.table)
rm(list=ls())
Sys.setenv(lang = "ENG")
Sys.setlocale("LC_ALL", "English") #not setting this to English will break as.Date()
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# as these pdf files are corrupt we delete them
file.remove('./data/executive_orders/00-31252.pdf')
file.remove('./data/executive_orders/2018-04860.pdf')
# function to read in data
read_pdfs <- function (folder_dir){
readtext(paste0('./data/', folder_dir),
docvarsfrom = "filenames",
docvarnames = 'document_number',
ignore_missing_files = TRUE,
verbosity = 3)
}
# read in all data
executive.orders <- read_pdfs('executive_orders/')
presidential.orders <- read_pdfs('presidential_orders/')
memorandums <- read_pdfs('memorandums/')
proclamations <- read_pdfs('proclamations/')
notices <- read_pdfs('notices/')
# merge with existing dfs to get date
executive.orders.df <- fread('./data/dataframes/executive_orders.csv')
presidential.orders.df <- fread('./data/dataframes/presidential_orders.csv')
memorandums.df <- fread('./data/dataframes/memorandums.csv')
proclamations.df <- fread('./data/dataframes/proclamations.csv')
notices.df <- fread('./data/dataframes/notices.csv')
# merge dataframes based on their document_number
executive.orders.df <- left_join(executive.orders.df, executive.orders, on = 'document_number') %>% select(-c(doc_id, abstract, excerpts, type))
presidential.orders.df <- left_join(presidential.orders.df, presidential.orders, on = 'document_number') %>% select(-c(doc_id, abstract, excerpts, type))
memorandums.df <- left_join(memorandums.df, memorandums, on = 'document_number') %>% select(-c(doc_id, abstract, excerpts, type))
proclamations.df <- left_join(proclamations.df, proclamations, on = 'document_number') %>% select(-c(doc_id, abstract, excerpts, type))
notices.df <- left_join(notices.df, notices, on = 'document_number') %>% select(-c(doc_id, abstract, excerpts, type))
# add variable
executive.orders.df$document_type <- 'executive.order'
presidential.orders.df$document_type <- 'presidential.order'
memorandums.df$document_type <- 'memorandum'
proclamations.df$document_type <- 'proclamation'
notices.df$document_type <- 'notice'
# merge all dataframes together
documents <- rbind(executive.orders.df, presidential.orders.df, memorandums.df, proclamations.df, notices.df)
documents <- documents[order(publication_date),]
# count how many NAs we have
sum(is.na(documents$text))
# remove unnecessary values
rm(executive.orders, memorandums, notices, presidential.orders, proclamations)
# check how many unique documents we have
length(unique(documents$document_number))
glimpse(documents)
# The following code creates the already cleaned main corpus for our analysis.
corp_main <- corpus(documents)
corp_main <- tokens(corp_main,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_remove(stopwords("english"))
number_corp_main<-ntoken(corp_main)
documents<-cbind(documents,number_corp_main)
corp1 <- corpus(documents)
month <- c("January", "February", "March", "April", "May", "June","July", "August", "September", "October", "November", "December")
day <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday","Sunday")
USA <- c("States", "Sec", "United","Act","Secretary","Council","State","Department","General","Section","Management","America","Committee","American","Americans","Washington")
tokens_corp1 <- tokens(corp1,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_remove(c(stopwords("english"),
month,
day,
USA))
toks_label <- tokens_lookup(tokens_corp1,
dictionary = data_dictionary_newsmap_en,
levels = 3) # level 3 stands for countries
dfmat_label <- dfm(toks_label, tolower = FALSE)
dfmat_feat <- dfm(tokens_corp1, tolower = FALSE)
dfmat_feat_select <- dfm_select(dfmat_feat, pattern = "^[A-Z][A-Za-z0-9]+",
valuetype = "regex", case_insensitive = FALSE) %>%
dfm_trim(min_termfreq = 10)
# Next, we will train the Newsmap model in a semi-supervised document classification approach, using the two document-feature matrices.
tmod_nm <- textmodel_newsmap(dfmat_feat_select, y = dfmat_label)
coef(tmod_nm,n=15)[c("US","CN","IQ")]
# The following code predicts the most strongly associated country for each EO.
pred_nm <- predict(tmod_nm)
head(pred_nm,1000)
# Here we get the frequency of countries in the Executive Orders
prediction_country<-table(pred_nm)
prediction_country
# The following code will join df1 with our predicted country labels.
documents<-cbind(documents,pred_nm)
View(documents)
# Next, we will train the Newsmap model in a semi-supervised document classification approach, using the two document-feature matrices.
tmod_nm <- textmodel_newsmap(dfmat_feat_select, y = dfmat_label)
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
library(newsmap)
library(sentimentr)
library(data.table)
coef(tmod_nm,n=15)[c("US","CN","IQ")]
