plot.top10 <- ggplot(top10, aes(x = frequency, y = reorder(country, frequency))) +
geom_bar(stat = 'identity') +
labs(title = 'Top 10 Frequency of countries (1950 -2021)',
y = '',
x = 'number of EOs',
subtitle = paste0('n = ', nrow(eo.top10))
) +
theme(plot.subtitle=element_text(size=9, hjust=0, face="italic", color="black"))
plot.top10
# plot counts over times
eo.top10$year <- year(eo.top10$date)
# count EOs per year and country
country.long <- eo.top10 %>% count(year, country)
# plot top 10 over time
plot.top10.time <- ggplot(country.long, aes(x=year, y=n, color = factor(country))) +
geom_line() +
facet_grid(rows = vars(reorder(country, -n)), scales = 'free') +
labs(title = 'Top 10 EOs counts over time (1950 -2021)',
y = '',
x = 'Years',
subtitle = paste0('n = ', nrow(eo.top10))
) +
theme(plot.subtitle=element_text(size=9, hjust=0, face="italic", color="black")) +
theme_bw() +
theme(legend.position = "none")
plot.top10.time
# with fixed
plot.top10.time <- ggplot(country.long, aes(x=year, y=n, color = factor(country))) +
geom_line() +
facet_grid(rows = vars(reorder(country, -n)), scales = 'fixed') +
labs(title = 'Top 10 EOs counts over time (1950 -2021)',
y = '',
x = 'Years',
subtitle = paste0('n = ', nrow(eo.top10))
) +
theme(plot.subtitle=element_text(size=9, hjust=0, face="italic", color="black")) +
theme_bw() +
theme(legend.position = "none")
plot.top10.time
### Save new dataframe ###
fwrite(data, './data/executive_orders_withcountry.csv')
# manually checking validity of the classification
checking_results <- data[sample(.N,6)]
# manually checking validity of the classification
checking_results <- data[sample(.N,30)]
View(checking_results)
# manually checking validity of the classification
set.seed(123)
checking_results <- data[sample(.N,30)]
View(checking_results)
checking_results$text
japan
View(checking_results)
checking_results <- data[sample(.N,30)]
checking_results$text
# manually checking validity of the classification
set.seed(123)
checking_results <- data[sample(.N,30)]
checking_results$text
View(checking_results)
library(quanteda)
library("quanteda.textplots")
library(data.table)
library(tidytext) #text mining, unnesting
library(topicmodels) #the LDA algorithm
library(tidyr) #gather()
library(dplyr) #awesome tools
library(ggplot2) #visualization
library(kableExtra) #create attractive tables
library(knitr) #simple table generator
library(ggrepel) #text and label geoms for ggplot2
library(gridExtra)
library(formattable) #color tile and color bar in `kables`
library(tm) #text mining
library(circlize) #already loaded, but just being comprehensive
library(plotly) #interactive ggplot graphs
library(stm)
rm(list=ls())
# set wd to where file is
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- fread('./data/executive_orders_withcountry.csv')
# download dictionary
download.file('http://www.snsoroka.com/wp-content/uploads/2020/08/LTDjun2013.zip',
destfile = './data/policy_agendas_english.zip')
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
lengths(lexicoder)
# analyze for china
target_countries <- c('China')
eo.china <- filter(data, country %in% target_countries)
eo.china.corpus <- corpus(eo.china,
docid_field =  "eo_number",
text_field = 'text')
# another approach
eo.dfm <- eo.china.corpus %>%
dfm(.,
tolower = TRUE,
remove = stopwords('english'),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE)
# what are most frequent used words
topfeatures(eo.dfm, 20)
# remove our own words
words <- c('united', 'states', 'president', 'america', 'presidency')
alphabet <- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'g', 'h')
lists <- c('i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x')
# try again
eo.dfm <- eo.china.corpus %>%
dfm(.,
tolower = TRUE,
remove = c(stopwords('english'), words, alphabet, lists),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE)
topfeatures(eo.dfm, 20)
# download dictionary
download.file('http://www.snsoroka.com/wp-content/uploads/2020/08/LTDjun2013.zip',
destfile = './data/policy_agendas_english.zip')
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
lengths(lexicoder)
eo.china.dfm <- eo.china.corpus %>%
dfm(.,
tolower = TRUE,
remove = c(stopwords('english'), words, alphabet, lists),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE,
dictionary = lexicoder)
head(eo.china.dfm)
eo.china.dfm <- eo.china.corpus %>%
dfm(.,
tolower = TRUE,
remove = c(stopwords('english'), words, alphabet, lists),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE,
dictionary = lexicoder)
library(lexicoder)
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
# download dictionary
download.file('http://www.snsoroka.com/wp-content/uploads/2020/08/LTDjun2013.zip',
destfile = './data/policy_agendas_english.zip')
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
lengths(lexicoder)
eo.china.dfm <- eo.china.corpus %>%
dfm(.,
tolower = TRUE,
remove = c(stopwords('english'), words, alphabet, lists),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE,
dictionary = lexicoder)
head(eo.china.dfm)
lengths(lexicoder)
eo.china.dfm <- eo.china.corpus %>%
dfm(.,
tolower = TRUE,
remove = c(stopwords('english'), words, alphabet, lists),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE,
dictionary = lexicoder)
head(eo.china.dfm)
# plot the most used topics
topics.df <- convert(eo.china.dfm, to = 'data.frame')
topics.df <- data.frame(colSums(topics.df[,-1], dims = 1))
topics.df$topic <- rownames(topics.df)
colnames(topics.df) <- c('sum' , 'topic')
topics.df
# filter all topics for which we have more than 100
topics.df <- topics.df[topics.df$sum >100, ]
# pie chart
ggplot(topics.df, aes(x="", y = sum, fill = topic)) +
geom_bar(width = 1, stat = "identity") +
coord_polar("y", start=0) +
theme_void() +
ggtitle('Top Topics with Lexicoder for China')
eo.china.dfm <- tokens(eo.china.corpus,
remove_punct =  TRUE,
remove_numbers = TRUE,
remove_separators = TRUE,
remove_symbols = TRUE,
remove_url = TRUE) %>%
tokens_remove(c(stopwords("en"), words)) %>%
dfm()
eo.china.dfm <- dfm_trim(eo.china.dfm, min_termfreq = 4, max_termfreq = 10)
if (require("stm")) {
my_lda_fit20 <- stm(eo.china.dfm, K = 20,
verbose = TRUE)
plot(my_lda_fit20)
}
# 3 Topic Modeling with Quanteda ----
#===================#
library(stm)
# 3 Topic Modeling with Quanteda ----
#===================#
install.packages("stm")
library(stm)
eo.china.dfm <- tokens(eo.china.corpus,
remove_punct =  TRUE,
remove_numbers = TRUE,
remove_separators = TRUE,
remove_symbols = TRUE,
remove_url = TRUE) %>%
tokens_remove(c(stopwords("en"), words)) %>%
dfm()
eo.china.dfm <- dfm_trim(eo.china.dfm, min_termfreq = 4, max_termfreq = 10)
if (require("stm")) {
my_lda_fit20 <- stm(eo.china.dfm, K = 20,
verbose = TRUE)
plot(my_lda_fit20)
}
lda <- LDA(data, k = 2, control = list(seed = 1234))
data("AssociatedPress")
myCorpus <- Corpus(VectorSource(data$text))
tdm <- TermDocumentMatrix(myCorpus)
# check which coefficients are associated to the individual countries
coef(tmod_nm,n=15)[c("US","CN","IQ", "IN")]
# make a corpus
eo.corpus <- corpus(data,
docid_field =  "eo_number",
text_field = 'text')
head(summary(eo.corpus))
# set some dictionaries for later
month <- c("January", "February", "March", "April", "May", "June","July", "August", "September", "October", "November", "December")
day <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday","Sunday")
USA <- c("United","America","American","Americans","Washington")
# create tokens
eo.tokens <- tokens(eo.corpus,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_remove(c(stopwords("english"),
month,
day,
USA))
# create labels
eo.label <- tokens_lookup(eo.tokens,
dictionary = data_dictionary_newsmap_en,
levels = 3) # level 3 stands for countries
# Document-feature matrix
dfmat_label <- dfm(eo.label, tolower = FALSE)
dfmat_feat <- dfm(eo.tokens, tolower = FALSE)
# select
dfmat_feat_select <- dfm_select(dfmat_feat, pattern = "^[A-Z][A-Za-z0-9]+",
valuetype = "regex", case_insensitive = FALSE) %>%
dfm_trim(min_termfreq = 10)
# train the newsmap textmodel
tmod_nm <- textmodel_newsmap(dfmat_feat_select, y = dfmat_label)
# check which coefficients are associated to the individual countries
coef(tmod_nm,n=15)[c("US","CN","IQ", "IN")]
library(quanteda)
library("quanteda.textplots")
library(data.table)
library(tidytext) #text mining, unnesting
library(topicmodels) #the LDA algorithm
library(tidyr) #gather()
library(dplyr) #awesome tools
library(ggplot2) #visualization
library(kableExtra) #create attractive tables
library(knitr) #simple table generator
library(ggrepel) #text and label geoms for ggplot2
library(gridExtra)
library(formattable) #color tile and color bar in `kables`
library(tm) #text mining
library(circlize) #already loaded, but just being comprehensive
library(plotly) #interactive ggplot graphs
library(stm)
rm(list=ls())
# set wd to where file is
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- fread('./data/executive_orders_withcountry.csv')
# download dictionary
download.file('http://www.snsoroka.com/wp-content/uploads/2020/08/LTDjun2013.zip',
destfile = './data/policy_agendas_english.zip')
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
lengths(lexicoder)
library(quanteda)
library("quanteda.textplots")
library(data.table)
library(tidytext) #text mining, unnesting
library(topicmodels) #the LDA algorithm
library(tidyr) #gather()
library(dplyr) #awesome tools
library(ggplot2) #visualization
library(kableExtra) #create attractive tables
library(knitr) #simple table generator
library(ggrepel) #text and label geoms for ggplot2
library(gridExtra)
library(formattable) #color tile and color bar in `kables`
library(tm) #text mining
library(circlize) #already loaded, but just being comprehensive
library(plotly) #interactive ggplot graphs
library(stm)
rm(list=ls())
# set wd to where file is
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- fread('./data/executive_orders_withcountry.csv')
# download dictionary
download.file('http://www.snsoroka.com/wp-content/uploads/2020/08/LTDjun2013.zip',
destfile = './data/policy_agendas_english.zip')
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
lengths(lexicoder)
# analyze for china
eo.countries <- filter(data, country)
View(data)
# set wd to where file is
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- fread('./data/executive_orders_cleaned.csv')
# download dictionary
download.file('http://www.snsoroka.com/wp-content/uploads/2020/08/LTDjun2013.zip',
destfile = './data/policy_agendas_english.zip')
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
lengths(lexicoder)
eo.corpus <- corpus(data,
docid_field =  "eo_number",
text_field = 'text')
# another approach
eo.dfm <- eo.corpus %>%
dfm(.,
tolower = TRUE,
remove = stopwords('english'),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE)
# what are most frequent used words
topfeatures(eo.dfm, 20)
# remove our own words
words <- c('united', 'states', 'president', 'america', 'presidency')
alphabet <- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'g', 'h')
lists <- c('i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x')
# try again
eo.dfm <- eo.corpus %>%
dfm(.,
tolower = TRUE,
remove = c(stopwords('english'), words, alphabet, lists),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE)
topfeatures(eo.dfm, 20)
# download dictionary
download.file('http://www.snsoroka.com/wp-content/uploads/2020/08/LTDjun2013.zip',
destfile = './data/policy_agendas_english.zip')
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
lengths(lexicoder)
library(quanteda)
library("quanteda.textplots")
library(data.table)
library(tidytext) #text mining, unnesting
library(topicmodels) #the LDA algorithm
library(tidyr) #gather()
library(dplyr) #awesome tools
library(ggplot2) #visualization
library(kableExtra) #create attractive tables
library(knitr) #simple table generator
library(ggrepel) #text and label geoms for ggplot2
library(gridExtra)
library(formattable) #color tile and color bar in `kables`
library(tm) #text mining
library(circlize) #already loaded, but just being comprehensive
library(plotly) #interactive ggplot graphs
library(stm)
rm(list=ls())
# set wd to where file is
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- fread('./data/executive_orders_withcountry.csv')
# download dictionary
download.file('http://www.snsoroka.com/wp-content/uploads/2020/08/LTDjun2013.zip',
destfile = './data/policy_agendas_english.zip')
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
lengths(lexicoder)
eo.corpus <- corpus(data,
docid_field =  "eo_number",
text_field = 'text')
eo.dfm <- eo.corpus %>%
dfm(.,
tolower = TRUE,
remove = c(stopwords('english'), words, alphabet, lists),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE,
dictionary = lexicoder)
head(eo.dfm)
# remove our own words
words <- c('united', 'states', 'president', 'america', 'presidency')
alphabet <- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'g', 'h')
lists <- c('i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x')
eo.dfm <- eo.corpus %>%
dfm(.,
tolower = TRUE,
remove = c(stopwords('english'), words, alphabet, lists),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE,
dictionary = lexicoder)
head(eo.dfm)
# plot the most used topics
topics.df <- convert(eo.dfm, to = 'data.frame')
topics.df <- data.frame(colSums(topics.df[,-1], dims = 1))
topics.df$topic <- rownames(topics.df)
colnames(topics.df) <- c('sum' , 'topic')
topics.df
# filter all topics for which we have more than 100
topics.df <- topics.df[topics.df$sum >100, ]
# pie chart
ggplot(topics.df, aes(x="", y = sum, fill = topic)) +
geom_bar(width = 1, stat = "identity") +
coord_polar("y", start=0) +
theme_void() +
ggtitle('Top Topics with Lexicoder')
# # compute the most relevant per document
topics.lexicoder <- convert(eo.dfm, to = 'data.frame')
topics.lexicoder <- topics.lexicoder %>% gather(key = 'topic', value = 'count', -doc_id)
length(unique(topics.lexicoder$doc_id))
data<-cbind(data,topics.lexicoder)
View(topics.lexicoder)
#
topics.lexicoder2 <- topics.lexicoder %>% group_by(doc_id) %>% top_n(1, count)
nrow(topics.lexicoder2) # we have some documents, for which we have equal counts
length(unique(topics.lexicoder2$doc_id))
rm(list=ls())
# set wd to where file is
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- fread('./data/executive_orders_withcountry.csv')
# download dictionary
download.file('http://www.snsoroka.com/wp-content/uploads/2020/08/LTDjun2013.zip',
destfile = './data/policy_agendas_english.zip')
# set up dictionary
lexicoder <- dictionary(file = './data/LTDjun2013/policy_agendas_english.lcd')
lengths(lexicoder)
eo.corpus <- corpus(data,
docid_field =  "eo_number",
text_field = 'text')
# remove our own words
words <- c('united', 'states', 'president', 'america', 'presidency')
alphabet <- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'g', 'h')
lists <- c('i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x')
eo.dfm <- eo.corpus %>%
dfm(.,
tolower = TRUE,
remove = c(stopwords('english'), words, alphabet, lists),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE,
dictionary = lexicoder)
head(eo.dfm)
# plot the most used topics
topics.df <- convert(eo.dfm, to = 'data.frame')
topics.df <- data.frame(colSums(topics.df[,-1], dims = 1))
topics.df$topic <- rownames(topics.df)
colnames(topics.df) <- c('sum' , 'topic')
topics.df
# filter all topics for which we have more than 100
topics.df <- topics.df[topics.df$sum >100, ]
# pie chart
ggplot(topics.df, aes(x="", y = sum, fill = topic)) +
geom_bar(width = 1, stat = "identity") +
coord_polar("y", start=0) +
theme_void() +
ggtitle('Top Topics with Lexicoder')
# # compute the most relevant per document
topics.lexicoder <- convert(eo.dfm, to = 'data.frame')
View(topics.lexicoder)
topics.lexicoder <- unique(topics.lexicoder$doc_id)
(unique(topics.lexicoder$doc_id))
# # compute the most relevant per document
topics.lexicoder <- convert(eo.dfm, to = 'data.frame')
topics.lexicoder <- topics.lexicoder %>% gather(key = 'topic', value = 'count', -doc_id)
