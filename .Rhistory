library(tidyr)
library(tidyselect)
library(tidyverse)
library(ggthemes)
library(nycflights13)
Sys.setenv(LANG = "eng")
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data = mpg)
ggplot(data = mpg) +
geom_point(mapping = aes(x = hwy, y = cyl))
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
ggplot(data = mpg) +
geom_point(mapping = aes(x = hwy, y = cyl))
ggplot(data = mpg) +
geom_point(mapping = aes(x = hwy, y = cyl))
library(tidyr)
library(tidyselect)
library(tidyverse)
library(ggthemes)
library(nycflights13)
Sys.setenv(LANG = "eng")
ggplot(data = flights, mapping = aes(dep_time, arr_delay)) +
geom_point()
library(tidyr)
library(tidyselect)
library(tidyverse)
library(ggthemes)
library(nycflights13)
Sys.setenv(LANG = "eng")
str(flights)
min_delay_daytime <- flights %>%
ggplot(data = flights, mapping = aes(dep_time, arr_delay)) +
geom_point() +
geom_line()
ggplot(data = flights, mapping = aes(dep_time, arr_delay)) +
geom_point() +
geom_line()
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
arrange(desc(avg_arr_delay)))
?n
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(n > 30)
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 30)
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(!is.na(count))
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 0)
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 0)
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 0)
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 0) %>%
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 0) %>%
arrange(avg_arr_delay))
View(combined_matrix)
View(diamonds2)
View(cars.subset)
search()
systemInfo()
systemInfo()
system("info")
system()
?system
search()
diamonds
diamonds2
View(diamonds2)
diamonds2
Sys.setenv(lang = "eng")
aetrsoin
diamonds2
library("vctrs")
diamonds
data()
swiss
attenu
?data
WorldPhones
cars
chickwts
co2
co2
head(co2)
str(co2)
library(dplyr)
glimpse(co2)
Nile
Titanic
str(Titanic)
glimpse(Titanic)
big_matrix
library(ggplot2)
data()
co2
chickwt
Nile
rock
mpg
ggplot(mpg, aes(cty, hwy))
ggplot(mpg, aes(cty, hwy)) +
geom_point(aes(label = drv))
geom_point(aes(drv = label))
ggplot(mpg, aes(cty, hwy)) +
geom_text(aes(label = year))
diamonds
library(httr)
library(jsonlite)
library(httr)
library(jsonlite)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 1000))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
#merge both dataframes
df <- full_join(df3, df4)
range(df$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(df3$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(df$publication_date < as.Date(min_date)), ]# it does
# download all pdfs
dir.create('./data')
for (i in 1:nrow(df)){
name = df$document_number[i]
download.file(df$pdf_url[i], destfile = paste0('./data/', name, '.pdf'))
}
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 1000))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
#merge both dataframes
df <- full_join(df3, df4)
range(df$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(df3$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(df$publication_date < as.Date(min_date)), ]# it does
# download all pdfs
dir.create('./data')
for (i in 1:nrow(df)){
name = df$document_number[i]
download.file(df$pdf_url[i], destfile = paste0('./data/', name, '.pdf'))
}
?res
?res
?res
source('~/GitHub/MBFQuantitativeTextAnalysis/01_FetchData.R', echo=TRUE)
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 1000))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
#merge both dataframes
df <- full_join(df3, df4)
range(df$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(df3$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(df$publication_date < as.Date(min_date)), ]# it does
# download all pdfs
dir.create('./data')
for (i in 1:nrow(df)){
name = df$document_number[i]
tryCatch(download(df$pdf_url[i], destfile = paste0('./data/', name, '.pdf'), timeout = 1000),
error = function(e) print(paste(name, e)))
}
# der geht lange
# tryCatch(download('https://www.govinfo.gov/content/pkg/FR-2018-03-08/pdf/2018-04860.pdf', destfile = paste0('./data2/', name, '.pdf'), timeout = 360),
#          error = function(e) print(paste(name, e)))
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 1000))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
#merge both dataframes
df <- full_join(df3, df4)
range(df$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(df3$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(df$publication_date < as.Date(min_date)), ]# it does
# download all pdfs
dir.create('./data')
for (i in 1:nrow(df)){
name = df$document_number[i]
tryCatch(download(df$pdf_url[i], destfile = paste0('./data/', name, '.pdf'), timeout = 1000),
error = function(e) print(paste(name, e)))
}
# der geht lange
# tryCatch(download('https://www.govinfo.gov/content/pkg/FR-2018-03-08/pdf/2018-04860.pdf', destfile = paste0('./data2/', name, '.pdf'), timeout = 360),
#          error = function(e) print(paste(name, e)))
install.packages(downloader)
install.packages("downloader")
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 1000))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
#merge both dataframes
df <- full_join(df3, df4)
range(df$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(df3$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(df$publication_date < as.Date(min_date)), ]# it does
# download all pdfs
dir.create('./data')
for (i in 1:nrow(df)){
name = df$document_number[i]
tryCatch(download(df$pdf_url[i], destfile = paste0('./data/', name, '.pdf'), timeout = 1000),
error = function(e) print(paste(name, e)))
}
# der geht lange
# tryCatch(download('https://www.govinfo.gov/content/pkg/FR-2018-03-08/pdf/2018-04860.pdf', destfile = paste0('./data2/', name, '.pdf'), timeout = 360),
#          error = function(e) print(paste(name, e)))
library(tidyverse)
library(dplyr)
library(httr)
library(jsonlite)
library(downloader)
rm(list=ls())
# set wd to where the source file is
# make sure you have the datafiles in a /data/ folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### FETCH DATA -----------------------------------
# https://www.federalregister.gov/developers/documentation/api/v1
# set the base url for the REST API
base = 'https://www.federalregister.gov/api/v1/'
# fetch all kinds of  documents
res = GET(paste0(base, 'documents.json'),
query = list(per_page = 1000))
data = fromJSON(rawToChar(res$content))
df <- data.frame(data)
unique(df$results.type) # there are four different types of documents
range(df$results.publication_date) # data only goes back 10 days
# get a single executive order
res = GET('https://www.federalregister.gov/api/v1/documents/2021-10139.json')
data2 = fromJSON(rawToChar(res$content))
# get all executive orders
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Btype%5D%5B%5D=PRESDOCU&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data3 = fromJSON(rawToChar((res$content)))
df3 <- data.frame(data3$results)
range(df3$publication_date) # data goes from 1997 to 2021
# as we can get maximum 1000 but there are more executive orders we make an additional request
res = GET('https://www.federalregister.gov/api/v1/documents.json?per_page=1000&conditions%5Bpublication_date%5D%5Blte%5D=2000-01-01&conditions%5Bpresidential_document_type%5D%5B%5D=executive_order')
data4 = fromJSON((rawToChar(res$content)))
df4 <- data.frame(data4$results)
#merge both dataframes
df <- full_join(df3, df4)
range(df$publication_date) # data goes from 1994 to 2021
# check if the merge made sense
min_date <- min(range(df3$publication_date)) # data goes from 1997 to 2021
missing_df <- df[which(df$publication_date < as.Date(min_date)), ]# it does
# download all pdfs
dir.create('./data')
for (i in 1:nrow(df)){
name = df$document_number[i]
tryCatch(download(df$pdf_url[i], destfile = paste0('./data/', name, '.pdf'), timeout = 1000),
error = function(e) print(paste(name, e)))
}
# der geht lange
# tryCatch(download('https://www.govinfo.gov/content/pkg/FR-2018-03-08/pdf/2018-04860.pdf', destfile = paste0('./data2/', name, '.pdf'), timeout = 360),
#          error = function(e) print(paste(name, e)))
