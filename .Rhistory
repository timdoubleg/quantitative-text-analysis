library(tidyr)
library(tidyselect)
library(tidyverse)
library(ggthemes)
library(nycflights13)
Sys.setenv(LANG = "eng")
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data =  mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data = mpg)
ggplot(data = mpg) +
geom_point(mapping = aes(x = hwy, y = cyl))
source('C:/Users/david/Desktop/R/R-for-data-science.R', echo=TRUE)
ggplot(data = mpg) +
geom_point(mapping = aes(x = hwy, y = cyl))
ggplot(data = mpg) +
geom_point(mapping = aes(x = hwy, y = cyl))
library(tidyr)
library(tidyselect)
library(tidyverse)
library(ggthemes)
library(nycflights13)
Sys.setenv(LANG = "eng")
ggplot(data = flights, mapping = aes(dep_time, arr_delay)) +
geom_point()
library(tidyr)
library(tidyselect)
library(tidyverse)
library(ggthemes)
library(nycflights13)
Sys.setenv(LANG = "eng")
str(flights)
min_delay_daytime <- flights %>%
ggplot(data = flights, mapping = aes(dep_time, arr_delay)) +
geom_point() +
geom_line()
ggplot(data = flights, mapping = aes(dep_time, arr_delay)) +
geom_point() +
geom_line()
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
arrange(desc(avg_arr_delay)))
?n
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(n > 30)
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 30)
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(!is.na(count))
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 0)
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 0)
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 0)
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 0) %>%
arrange(desc(avg_arr_delay)))
(min_delay_daytime <- flights %>%
group_by(hour) %>%
summarise(
count = n(),
avg_arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
filter(count > 0) %>%
arrange(avg_arr_delay))
View(combined_matrix)
View(diamonds2)
View(cars.subset)
search()
systemInfo()
systemInfo()
system("info")
system()
?system
search()
diamonds
diamonds2
View(diamonds2)
diamonds2
Sys.setenv(lang = "eng")
aetrsoin
diamonds2
library("vctrs")
diamonds
data()
swiss
attenu
?data
WorldPhones
cars
chickwts
co2
co2
head(co2)
str(co2)
library(dplyr)
glimpse(co2)
Nile
Titanic
str(Titanic)
glimpse(Titanic)
big_matrix
library(ggplot2)
data()
co2
chickwt
Nile
rock
mpg
ggplot(mpg, aes(cty, hwy))
ggplot(mpg, aes(cty, hwy)) +
geom_point(aes(label = drv))
geom_point(aes(drv = label))
ggplot(mpg, aes(cty, hwy)) +
geom_text(aes(label = year))
diamonds
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
rm(list=ls())
Sys.setenv(lang = "ENG")
Sys.setlocale("LC_ALL", "English") #not setting this to English will break as.Date()
# 2 Data import ----
#===================#
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
#The filename correspond with the federal register doc id. They do not provide additional information, thus no further docvars are specified.
# Some PDFs like 08-62.pdf contain extensive parts with scanned text.
#The code below extractsthe pattern "Executive Order <nr> of <month> <nr>, <year>" from the texts
#It then extracts the EO number and the date of issuance and adds these as variables to a new dataframe df1
find_EO_dates <- function(data,
regex_pattern = "Executive\\s{1}Order\\s{1}\\d{4,6}\\s{1}of\\s{1}(January|February|March|April|May|June|July|August|September|October|November|December)\\s{1}\\d{1,2},\\s{1}\\d{4}") {
mutate(data,
EO_nr =
str_extract(text, regex_pattern) %>%
str_extract("\\d{4,6}") %>%
as.numeric(),
date =
str_extract(text, regex_pattern) %>%
str_extract("(January|February|March|April|May|June|July|August|September|October|November|December)\\s{1}\\d{1,2},\\s{1}\\d{4}") %>%
as.Date(format = "%B %d, %Y"))
}
df1 <- df1 %>% find_EO_dates()
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
library(here)
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
rm(list=ls())
Sys.setenv(lang = "ENG")
Sys.setlocale("LC_ALL", "English") #not setting this to English will break as.Date()
# 2 Data import ----
#===================#
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
rm(list=ls())
Sys.setenv(lang = "ENG")
Sys.setlocale("LC_ALL", "English") #not setting this to English will break as.Date()
# 2 Data import ----
#===================#
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
rm(list=ls())
Sys.setenv(lang = "ENG")
Sys.setlocale("LC_ALL", "English") #not setting this to English will break as.Date()
# 2 Data import ----
#===================#
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
library(readtext)
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
rm(list=ls())
Sys.setenv(lang = "ENG")
Sys.setlocale("LC_ALL", "English") #not setting this to English will break as.Date()
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
list.files()
get.wd()
getwd()
?here
setwd(D:/Documents/GitHub/MBFQuantitativeTextAnalysis)
setwd("D:/Documents/GitHub/MBFQuantitativeTextAnalysis")
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
getwd
getwd()
list_files()
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
library(here)
library(readtext)
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
rm(list=ls())
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
rm(list=ls())
Sys.setenv(lang = "ENG")
Sys.setlocale("LC_ALL", "English") #not setting this to English will break as.Date()
# 2 Data import ----
#===================#
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
rm(list=ls())
Sys.setenv(lang = "ENG")
Sys.setlocale("LC_ALL", "English") #not setting this to English will break as.Date()
# 2 Data import ----
#===================#
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
find_EO_dates <- function(data,
regex_pattern = "Executive\\s{1}Order\\s{1}\\d{4,6}\\s{1}of\\s{1}(January|February|March|April|May|June|July|August|September|October|November|December)\\s{1}\\d{1,2},\\s{1}\\d{4}") {
mutate(data,
EO_nr =
str_extract(text, regex_pattern) %>%
str_extract("\\d{4,6}") %>%
as.numeric(),
date =
str_extract(text, regex_pattern) %>%
str_extract("(January|February|March|April|May|June|July|August|September|October|November|December)\\s{1}\\d{1,2},\\s{1}\\d{4}") %>%
as.Date(format = "%B %d, %Y"))
}
df1 <- df1 %>% find_EO_dates()
library(here)
library(readtext)
library(quanteda)
library(stringr)
library(dplyr)
rm(list=ls())
Sys.setenv(lang = "ENG")
Sys.setlocale("LC_ALL", "English") #not setting this to English will break as.Date()
# 2 Data import ----
#===================#
## Importing texts ----
df1 <- readtext(here("data"),
docvarsfrom = "filenames")
glimpse(df1)
df1 <- readtext(here("data"))
glimpse(df1)
find_EO_dates <- function(data,
regex_pattern = "Executive\\s{1}Order\\s{1}\\d{4,6}\\s{1}of\\s{1}(January|February|March|April|May|June|July|August|September|October|November|December)\\s{1}\\d{1,2},\\s{1}\\d{4}") {
mutate(data,
EO_nr =
str_extract(text, regex_pattern) %>%
str_extract("\\d{4,6}") %>%
as.numeric(),
date =
str_extract(text, regex_pattern) %>%
str_extract("(January|February|March|April|May|June|July|August|September|October|November|December)\\s{1}\\d{1,2},\\s{1}\\d{4}") %>%
as.Date(format = "%B %d, %Y"))
}
df1 <- df1 %>% find_EO_dates()
glimpse(df1)
nrow(df1) #there are 1108 documents
range(nchar(df1$EO_nr), na.rm = TRUE) #all EO_nr have the same length, as they should
sum(is.na(df1$EO_nr)) #there are 25 missing EO_nr values
sum(is.na(df1$date)) #there are 25 missing date values
sum(is.na(df1$EO_nr) | is.na(df1$date)) #25 indicates that the documents missing EO_nr are also the ones missing date values
df1 %>% filter(is.na(date)) %>% data.frame() %>%  select(doc_id) #displays a df that contains
df2 <- df1 %>% ## I don't understand why this code is not working! all the dates remain NA ----
filter(is.na(date)) %>%
find_EO_dates(regex_pattern =  = new_regex)
sum(is.na(df1$date)) # there are still 25 missing date values, meaning the code above failed
df2 <- df1 %>% ## I don't understand why this code is not working! all the dates remain NA ----
filter(is.na(date)) %>%
find_EO_dates(regex_pattern = new_regex)
sum(is.na(df1$date)) # there are still 25 missing date values, meaning the code above failed
### Adapting the regex ----
new_regex <- "Order\\s{1}of\\s{1}(January|February|March|April|May|June|July|August|September|October|November|December)\\s{1}\\d{1,2},\\s{1}\\d{4}"
df2 <- df1 %>% ## I don't understand why this code is not working! all the dates remain NA ----
filter(is.na(date)) %>%
find_EO_dates(regex_pattern = new_regex)
sum(is.na(df1$date)) # there are still 25 missing date values, meaning the code above failed
df1 %>% ## meanwhile, picking a single document like this somehow works, the date is correctly extracted. I don't know why  ----
filter(is.na(date)) %>%
filter(doc_id == "2016-29494.pdf") %>%
select(text) %>%
str_extract(new_regex) %>%
str_extract("(January|February|March|April|May|June|July|August|September|October|November|December)\\s{1}\\d{1,2},\\s{1}\\d{4}") %>%
as.Date(format = "%B %d, %Y")
corp_main <- corpus(df1)
number_corp_main<-ntoken(corp_main)
df1<-cbind(df1,number_corp_main)
glimpse(df1)
corp1 <- corpus(df1)
month <- c("January", "February", "March", "April", "May", "June","July", "August", "September", "October", "November", "December")
day <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday","Sunday")
USA <- c("States", "Sec", "United","Act","Secretary","Council","State","Department","General","Section","Management","America","Committee","American","Americans","Washington")
tokens_corp1 <- tokens(corp1, remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_remove(c(stopwords("english"),
month,
day,
USA))
toks_label <- tokens_lookup(tokens_corp1, dictionary = data_dictionary_newsmap_en,
levels = 3) # level 3 stands for countries
dfmat_label <- dfm(toks_label, tolower = FALSE)
dfmat_feat <- dfm(tokens_corp1, tolower = FALSE)
dfmat_feat_select <- dfm_select(dfmat_feat, pattern = "^[A-Z][A-Za-z0-9]+",
valuetype = "regex", case_insensitive = FALSE) %>%
dfm_trim(min_termfreq = 10)
tmod_nm <- textmodel_newsmap(dfmat_feat_select, y = dfmat_label)
coef(tmod_nm,n=15)[c("US","CN")]
install.packages(newsmap)
install.packages("newsmap")
library(newsmap)
toks_label <- tokens_lookup(tokens_corp1, dictionary = data_dictionary_newsmap_en,
levels = 3) # level 3 stands for countries
dfmat_label <- dfm(toks_label, tolower = FALSE)
dfmat_feat <- dfm(tokens_corp1, tolower = FALSE)
dfmat_feat_select <- dfm_select(dfmat_feat, pattern = "^[A-Z][A-Za-z0-9]+",
valuetype = "regex", case_insensitive = FALSE) %>%
dfm_trim(min_termfreq = 10)
tmod_nm <- textmodel_newsmap(dfmat_feat_select, y = dfmat_label)
coef(tmod_nm,n=15)[c("US","CN")]
pred_nm <- predict(tmod_nm)
pred_nm <- predict(tmod_nm)
head(pred_nm,1000)
coef(tmod_nm,n=15)[c("US","CN")]
prediction_country<-table(pred_nm)
prediction_country
coef(tmod_nm,n=15)[c("US","CN","IN")] #gives numeric measure of a word's relevance to its
